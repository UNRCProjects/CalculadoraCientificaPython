import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from backend.series_temporales import calcular_metricas

def render():
    st.header("üìà M√©tricas de Evaluaci√≥n")
    st.markdown("""
    **Eval√∫a el rendimiento** del modelo ARIMA usando diferentes m√©tricas estad√≠sticas.
    Estas m√©tricas te ayudan a entender qu√© tan bien predice el modelo.
    """)
    
    # Verificar si hay predicciones disponibles
    if not st.session_state.get('predicciones_generadas', False):
        st.warning("‚ö†Ô∏è Primero debes generar predicciones en la secci√≥n 'Predicciones'")
        return
    
    predicciones = st.session_state['predicciones_arima']
    datos = st.session_state['datos_ventas']
    n_periodos = st.session_state['n_periodos_pred']
    
    # Usar los √∫ltimos n_periodos de datos reales para comparar
    datos_reales = datos.tail(n_periodos)
    datos_predichos = predicciones['prediccion']
    
    st.subheader("üìä M√©tricas de Precisi√≥n")
    
    if st.button("Calcular M√©tricas", key="calcular_metricas"):
        with st.spinner("Calculando m√©tricas..."):
            # Calcular m√©tricas
            metricas = calcular_metricas(datos_reales, datos_predichos)
            
            # Mostrar m√©tricas principales
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("RMSE", f"{metricas['RMSE']:.2f}")
                st.caption("Root Mean Square Error")
            with col2:
                st.metric("MAE", f"{metricas['MAE']:.2f}")
                st.caption("Mean Absolute Error")
            with col3:
                st.metric("MAPE", f"{metricas['MAPE']:.1f}%")
                st.caption("Mean Absolute Percentage Error")
            with col4:
                st.metric("MSE", f"{metricas['MSE']:.2f}")
                st.caption("Mean Square Error")
            
            # Guardar m√©tricas en session_state
            st.session_state['metricas_arima'] = metricas
            st.session_state['metricas_calculadas'] = True
            
            # Interpretaci√≥n de m√©tricas
            st.subheader("üí° Interpretaci√≥n de M√©tricas")
            
            # RMSE
            if metricas['RMSE'] < datos_reales.std():
                st.success("‚úÖ **RMSE Bueno** - El error es menor que la variabilidad natural")
            elif metricas['RMSE'] < datos_reales.std() * 2:
                st.warning("‚ö†Ô∏è **RMSE Moderado** - El error es aceptable")
            else:
                st.error("‚ùå **RMSE Alto** - El modelo necesita mejoras")
            
            # MAPE
            if metricas['MAPE'] < 5:
                st.success("‚úÖ **MAPE Excelente** - Error menor al 5%")
            elif metricas['MAPE'] < 10:
                st.success("‚úÖ **MAPE Bueno** - Error menor al 10%")
            elif metricas['MAPE'] < 20:
                st.warning("‚ö†Ô∏è **MAPE Moderado** - Error entre 10-20%")
            else:
                st.error("‚ùå **MAPE Alto** - Error mayor al 20%")
            
            # MAE
            if metricas['MAE'] < datos_reales.mean() * 0.1:
                st.success("‚úÖ **MAE Excelente** - Error menor al 10% del promedio")
            elif metricas['MAE'] < datos_reales.mean() * 0.2:
                st.warning("‚ö†Ô∏è **MAE Moderado** - Error entre 10-20% del promedio")
            else:
                st.error("‚ùå **MAE Alto** - Error mayor al 20% del promedio")
            
            # Gr√°fico de comparaci√≥n
            st.subheader("üìä Comparaci√≥n: Real vs Predicho")
            
            fig = go.Figure()
            
            # Datos reales
            fig.add_trace(go.Scatter(
                x=datos_reales.index,
                y=datos_reales.values,
                mode='lines+markers',
                name='Datos Reales',
                line=dict(width=2, color='#3498db'),
                marker=dict(size=6)
            ))
            
            # Datos predichos
            fechas_pred = pd.date_range(start=datos_reales.index[0], periods=len(datos_predichos), freq='D')
            fig.add_trace(go.Scatter(
                x=fechas_pred,
                y=datos_predichos.values,
                mode='lines+markers',
                name='Predicciones',
                line=dict(width=2, color='#e74c3c', dash='dash'),
                marker=dict(size=6)
            ))
            
            fig.update_layout(
                title="Comparaci√≥n: Datos Reales vs Predicciones",
                xaxis_title="Fecha",
                yaxis_title="Ventas ($)",
                hovermode='x unified',
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # An√°lisis de errores
            st.subheader("üîç An√°lisis de Errores")
            
            errores = datos_reales.values - datos_predichos.values
            errores_abs = np.abs(errores)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Distribuci√≥n de Errores:**")
                st.markdown(f"‚Ä¢ Error promedio: ${np.mean(errores):,.0f}")
                st.markdown(f"‚Ä¢ Error m√°ximo: ${np.max(errores_abs):,.0f}")
                st.markdown(f"‚Ä¢ Error m√≠nimo: ${np.min(errores_abs):,.0f}")
                st.markdown(f"‚Ä¢ Desviaci√≥n est√°ndar: ${np.std(errores):,.0f}")
            
            with col2:
                st.markdown("**Calidad del Modelo:**")
                if metricas['MAPE'] < 10:
                    st.success("üü¢ **Excelente**")
                elif metricas['MAPE'] < 20:
                    st.warning("üü° **Bueno**")
                else:
                    st.error("üî¥ **Necesita Mejoras**")
                
                if metricas['RMSE'] < datos_reales.std():
                    st.success("üü¢ **Preciso**")
                else:
                    st.warning("üü° **Moderado**")
            
            # Recomendaciones
            st.subheader("üéØ Recomendaciones")
            
            if metricas['MAPE'] < 10 and metricas['RMSE'] < datos_reales.std():
                st.success("""
                **üéâ Modelo Excelente:**
                - El modelo tiene muy buena precisi√≥n
                - Puedes confiar en las predicciones
                - Considera usar este modelo para decisiones de negocio
                """)
            elif metricas['MAPE'] < 20:
                st.warning("""
                **‚ö†Ô∏è Modelo Aceptable:**
                - El modelo tiene precisi√≥n moderada
                - √ösalo con precauci√≥n para decisiones importantes
                - Considera ajustar los par√°metros (p,d,q)
                """)
            else:
                st.error("""
                **‚ùå Modelo Necesita Mejoras:**
                - El modelo tiene baja precisi√≥n
                - No conf√≠es en las predicciones
                - Prueba diferentes par√°metros o m√©todos
                - Considera m√°s datos de entrenamiento
                """)
    
    # Mostrar informaci√≥n sobre m√©tricas si no est√°n calculadas
    if not st.session_state.get('metricas_calculadas', False):
        st.subheader("üìö ¬øQu√© son las M√©tricas de Evaluaci√≥n?")
        st.markdown("""
        **Las m√©tricas de evaluaci√≥n** miden qu√© tan bien predice el modelo:
        
        **RMSE (Root Mean Square Error):**
        - Mide la ra√≠z cuadrada del error cuadr√°tico medio
        - Penaliza m√°s los errores grandes
        - Misma unidad que los datos originales
        
        **MAE (Mean Absolute Error):**
        - Mide el error absoluto promedio
        - No penaliza m√°s los errores grandes
        - M√°s f√°cil de interpretar
        
        **MAPE (Mean Absolute Percentage Error):**
        - Mide el error porcentual promedio
        - Permite comparar entre diferentes escalas
        - Valores menores al 10% se consideran buenos
        
        **MSE (Mean Square Error):**
        - Mide el error cuadr√°tico medio
        - Penaliza mucho los errores grandes
        - Usado para optimizaci√≥n del modelo
        """)
